{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ad59464a7a794481ad79d847454eaeb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_51c90702ca8247b3b5185fbcc7237d53",
              "IPY_MODEL_649700260eb941b69153b610c0987268",
              "IPY_MODEL_72417829d97f4c19a311c5b5347e53aa"
            ],
            "layout": "IPY_MODEL_0e9d618aa77d4b04922e59017caa0e6d"
          }
        },
        "51c90702ca8247b3b5185fbcc7237d53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66ce74ff5fbb4e0bbb2bfec02015cebd",
            "placeholder": "​",
            "style": "IPY_MODEL_95a1cc895dcf4f4786d25696ee4dc68b",
            "value": "100%"
          }
        },
        "649700260eb941b69153b610c0987268": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba491fc69141487b8484ccb2bf4fda17",
            "max": 102530333,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_660e7914949549d4a5bfeb96bc2c22f4",
            "value": 102530333
          }
        },
        "72417829d97f4c19a311c5b5347e53aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f145e7ce1aed411bb349b914b59a1ed2",
            "placeholder": "​",
            "style": "IPY_MODEL_0b19fd438b424a0db1ccc25b5ded3422",
            "value": " 97.8M/97.8M [00:01&lt;00:00, 78.5MB/s]"
          }
        },
        "0e9d618aa77d4b04922e59017caa0e6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66ce74ff5fbb4e0bbb2bfec02015cebd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95a1cc895dcf4f4786d25696ee4dc68b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba491fc69141487b8484ccb2bf4fda17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "660e7914949549d4a5bfeb96bc2c22f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f145e7ce1aed411bb349b914b59a1ed2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b19fd438b424a0db1ccc25b5ded3422": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "n_zY50DYLD04"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0HANHc6HgKoj"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import multiprocessing\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils import model_zoo\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import models, transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torch.nn.functional as F\n",
        "import gdown\n",
        "import os.path\n",
        "from os import path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataloader for ImageNet\n",
        "def loader_imgnet(dir_data, nb_images = 50000, batch_size = 100, img_size = 224):\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(img_size),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    \n",
        "    val_dataset = ImageFolder(dir_data, val_transform)\n",
        "    \n",
        "    # Random subset if not using the full 50,000 validation set\n",
        "    if nb_images < 50000:\n",
        "        np.random.seed(0)\n",
        "        sample_indices = np.random.permutation(range(50000))[:nb_images]\n",
        "        val_dataset = Subset(val_dataset, sample_indices)\n",
        "    \n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size = batch_size,                              \n",
        "        shuffle = True, \n",
        "        num_workers = max(1, multiprocessing.cpu_count() - 1)\n",
        "    )\n",
        "    \n",
        "    return dataloader\n",
        "\n",
        "\n",
        "# dataloader for CIFAR-10\n",
        "def loader_cifar(dir_data, train = False, batch_size = 250):\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "    if train:\n",
        "        trainset = torchvision.datasets.CIFAR10(root = dir_data, train = True, download = True, transform = transform_test)\n",
        "        dataloader = torch.utils.data.DataLoader(trainset, batch_size = batch_size, shuffle = True, num_workers = max(1, multiprocessing.cpu_count() - 1))\n",
        "    else:\n",
        "        testset = torchvision.datasets.CIFAR10(root = dir_data, train = False, download = True, transform = transform_test)\n",
        "        dataloader = torch.utils.data.DataLoader(testset, batch_size = batch_size, shuffle = True, num_workers = max(1, multiprocessing.cpu_count() - 1))\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "# Evaluate model on data with or without UAP\n",
        "# Assumes data range is bounded by [0, 1]\n",
        "def evaluate(model, loader, uap = None, n = 5):\n",
        "    '''\n",
        "    OUTPUT\n",
        "    top         top n predicted labels (default n = 5)\n",
        "    top_probs   top n probabilities (default n = 5)\n",
        "    top1acc     array of true/false if true label is in top 1 prediction\n",
        "    top5acc     array of true/false if true label is in top 5 prediction\n",
        "    outputs     output labels\n",
        "    labels      true labels\n",
        "    '''\n",
        "    probs, labels = [], []\n",
        "    model.eval()\n",
        "    \n",
        "    if uap is not None:\n",
        "        _, (x_val, y_val) = next(enumerate(loader))\n",
        "        batch_size = len(x_val)\n",
        "        uap = uap.unsqueeze(0).repeat([batch_size, 1, 1, 1])\n",
        "    \n",
        "    with torch.set_grad_enabled(False):\n",
        "        for i, (x_val, y_val) in enumerate(loader):\n",
        "            \n",
        "            if uap is None:\n",
        "                out = torch.nn.functional.softmax(model(x_val.cuda()), dim = 1)\n",
        "            else:\n",
        "                perturbed = torch.clamp((x_val + uap).cuda(), 0, 1) # clamp to [0, 1]\n",
        "                out = torch.nn.functional.softmax(model(perturbed), dim = 1)\n",
        "                \n",
        "            probs.append(out.cpu().numpy())\n",
        "            labels.append(y_val)\n",
        "            \n",
        "    # Convert batches to single numpy arrays    \n",
        "    probs = np.stack([p for l in probs for p in l])\n",
        "    labels = np.array([t for l in labels for t in l])\n",
        "    \n",
        "    # Extract top 5 predictions for each example\n",
        "    top = np.argpartition(-probs, n, axis = 1)[:,:n]\n",
        "    top_probs = probs[np.arange(probs.shape[0])[:, None], top].astype(np.float16)\n",
        "    top1acc = top[range(len(top)), np.argmax(top_probs, axis = 1)] == labels\n",
        "    top5acc = [labels[i] in row for i, row in enumerate(top)]\n",
        "    outputs = top[range(len(top)), np.argmax(top_probs, axis = 1)]\n",
        "        \n",
        "    return top, top_probs, top1acc, top5acc, outputs, labels\n"
      ],
      "metadata": {
        "id": "UabBj5YqgO3I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, in_planes, growth_rate):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(4*growth_rate)\n",
        "        self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.bn1(x)))\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\n",
        "        out = torch.cat([out,x], 1)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Transition(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes):\n",
        "        super(Transition, self).__init__()\n",
        "        self.bn = nn.BatchNorm2d(in_planes)\n",
        "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(F.relu(self.bn(x)))\n",
        "        out = F.avg_pool2d(out, 2)\n",
        "        return out\n",
        "\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=10):\n",
        "        super(DenseNet, self).__init__()\n",
        "        self.growth_rate = growth_rate\n",
        "\n",
        "        num_planes = 2*growth_rate\n",
        "        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n",
        "        num_planes += nblocks[0]*growth_rate\n",
        "        out_planes = int(math.floor(num_planes*reduction))\n",
        "        self.trans1 = Transition(num_planes, out_planes)\n",
        "        num_planes = out_planes\n",
        "\n",
        "        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n",
        "        num_planes += nblocks[1]*growth_rate\n",
        "        out_planes = int(math.floor(num_planes*reduction))\n",
        "        self.trans2 = Transition(num_planes, out_planes)\n",
        "        num_planes = out_planes\n",
        "\n",
        "        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n",
        "        num_planes += nblocks[2]*growth_rate\n",
        "        out_planes = int(math.floor(num_planes*reduction))\n",
        "        self.trans3 = Transition(num_planes, out_planes)\n",
        "        num_planes = out_planes\n",
        "\n",
        "        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n",
        "        num_planes += nblocks[3]*growth_rate\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(num_planes)\n",
        "        self.linear = nn.Linear(num_planes, num_classes)\n",
        "\n",
        "    def _make_dense_layers(self, block, in_planes, nblock):\n",
        "        layers = []\n",
        "        for i in range(nblock):\n",
        "            layers.append(block(in_planes, self.growth_rate))\n",
        "            in_planes += self.growth_rate\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.trans1(self.dense1(out))\n",
        "        out = self.trans2(self.dense2(out))\n",
        "        out = self.trans3(self.dense3(out))\n",
        "        out = self.dense4(out)\n",
        "        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def DenseNet121():\n",
        "    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=32)\n",
        "\n",
        "def DenseNet169():\n",
        "    return DenseNet(Bottleneck, [6,12,32,32], growth_rate=32)\n",
        "\n",
        "def DenseNet201():\n",
        "    return DenseNet(Bottleneck, [6,12,48,32], growth_rate=32)\n",
        "\n",
        "def DenseNet161():\n",
        "    return DenseNet(Bottleneck, [6,12,36,24], growth_rate=48)\n",
        "\n",
        "def densenet_cifar():\n",
        "    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=12)"
      ],
      "metadata": {
        "id": "ySjd_pW9j6tQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    '''expand + depthwise + pointwise + squeeze-excitation'''\n",
        "\n",
        "    def __init__(self, in_planes, out_planes, expansion, stride):\n",
        "        super(Block, self).__init__()\n",
        "        self.stride = stride\n",
        "\n",
        "        planes = expansion * in_planes\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, groups=planes, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride == 1 and in_planes != out_planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, out_planes, kernel_size=1,\n",
        "                          stride=1, padding=0, bias=False),\n",
        "                nn.BatchNorm2d(out_planes),\n",
        "            )\n",
        "\n",
        "        # SE layers\n",
        "        self.fc1 = nn.Conv2d(out_planes, out_planes//16, kernel_size=1)\n",
        "        self.fc2 = nn.Conv2d(out_planes//16, out_planes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        shortcut = self.shortcut(x) if self.stride == 1 else out\n",
        "        # Squeeze-Excitation\n",
        "        w = F.avg_pool2d(out, out.size(2))\n",
        "        w = F.relu(self.fc1(w))\n",
        "        w = self.fc2(w).sigmoid()\n",
        "        out = out * w + shortcut\n",
        "        return out\n",
        "\n",
        "\n",
        "class EfficientNet(nn.Module):\n",
        "    def __init__(self, cfg, num_classes=10):\n",
        "        super(EfficientNet, self).__init__()\n",
        "        self.cfg = cfg\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.layers = self._make_layers(in_planes=32)\n",
        "        self.linear = nn.Linear(cfg[-1][1], num_classes)\n",
        "\n",
        "    def _make_layers(self, in_planes):\n",
        "        layers = []\n",
        "        for expansion, out_planes, num_blocks, stride in self.cfg:\n",
        "            strides = [stride] + [1]*(num_blocks-1)\n",
        "            for stride in strides:\n",
        "                layers.append(Block(in_planes, out_planes, expansion, stride))\n",
        "                in_planes = out_planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layers(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def EfficientNetB0():\n",
        "    # (expansion, out_planes, num_blocks, stride)\n",
        "    cfg = [(1,  16, 1, 2),\n",
        "           (6,  24, 2, 1),\n",
        "           (6,  40, 2, 2),\n",
        "           (6,  80, 3, 2),\n",
        "           (6, 112, 3, 1),\n",
        "           (6, 192, 4, 2),\n",
        "           (6, 320, 1, 2)]\n",
        "    return EfficientNet(cfg)"
      ],
      "metadata": {
        "id": "BUq3qDQ8j8qT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Inception(nn.Module):\n",
        "    def __init__(self, in_planes, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes):\n",
        "        super(Inception, self).__init__()\n",
        "        # 1x1 conv branch\n",
        "        self.b1 = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, n1x1, kernel_size=1),\n",
        "            nn.BatchNorm2d(n1x1),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "        # 1x1 conv -> 3x3 conv branch\n",
        "        self.b2 = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, n3x3red, kernel_size=1),\n",
        "            nn.BatchNorm2d(n3x3red),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(n3x3red, n3x3, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n3x3),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "        # 1x1 conv -> 5x5 conv branch\n",
        "        self.b3 = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, n5x5red, kernel_size=1),\n",
        "            nn.BatchNorm2d(n5x5red),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(n5x5red, n5x5, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n5x5),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(n5x5, n5x5, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n5x5),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "        # 3x3 pool -> 1x1 conv branch\n",
        "        self.b4 = nn.Sequential(\n",
        "            nn.MaxPool2d(3, stride=1, padding=1),\n",
        "            nn.Conv2d(in_planes, pool_planes, kernel_size=1),\n",
        "            nn.BatchNorm2d(pool_planes),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y1 = self.b1(x)\n",
        "        y2 = self.b2(x)\n",
        "        y3 = self.b3(x)\n",
        "        y4 = self.b4(x)\n",
        "        return torch.cat([y1,y2,y3,y4], 1)\n",
        "\n",
        "\n",
        "class GoogLeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GoogLeNet, self).__init__()\n",
        "        self.pre_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 192, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "        self.a3 = Inception(192,  64,  96, 128, 16, 32, 32)\n",
        "        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n",
        "\n",
        "        self.a4 = Inception(480, 192,  96, 208, 16,  48,  64)\n",
        "        self.b4 = Inception(512, 160, 112, 224, 24,  64,  64)\n",
        "        self.c4 = Inception(512, 128, 128, 256, 24,  64,  64)\n",
        "        self.d4 = Inception(512, 112, 144, 288, 32,  64,  64)\n",
        "        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)\n",
        "\n",
        "        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)\n",
        "        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)\n",
        "\n",
        "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
        "        self.linear = nn.Linear(1024, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pre_layers(x)\n",
        "        out = self.a3(out)\n",
        "        out = self.b3(out)\n",
        "        out = self.maxpool(out)\n",
        "        out = self.a4(out)\n",
        "        out = self.b4(out)\n",
        "        out = self.c4(out)\n",
        "        out = self.d4(out)\n",
        "        out = self.e4(out)\n",
        "        out = self.maxpool(out)\n",
        "        out = self.a5(out)\n",
        "        out = self.b5(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "InSFOergkB6-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2,2,2,2])\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3,4,6,3])\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3,4,6,3])\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3,4,23,3])\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3,8,36,3])"
      ],
      "metadata": {
        "id": "b4r_FWjzkFAB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "ge0le0jSkIaM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CIFAR_MEAN = [0.4914, 0.4822, 0.4465]\n",
        "CIFAR_STD = [0.2023, 0.1994, 0.2010]\n",
        "\n",
        "IMGNET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMGNET_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "\n",
        "class Normalizer(nn.Module):\n",
        "    def __init__(self, mean, std):\n",
        "        super(Normalizer, self).__init__()\n",
        "        if not isinstance(mean, torch.Tensor):\n",
        "            mean = torch.tensor(mean)\n",
        "        if not isinstance(std, torch.Tensor):\n",
        "            std = torch.tensor(std)\n",
        "        self.register_buffer(\"mean\", mean)\n",
        "        self.register_buffer(\"std\", std)\n",
        "        \n",
        "    def forward(self, tensor):\n",
        "        return normalize_fn(tensor, self.mean, self.std)\n",
        "    \n",
        "    def extra_repr(self):\n",
        "        return 'mean={}, std={}'.format(self.mean, self.std)\n",
        "    \n",
        "\n",
        "def normalize_fn(tensor, mean, std):\n",
        "    \"\"\"\n",
        "    Differentiable version of torchvision.functional.normalize\n",
        "    - default assumes color channel is at dim = 1\n",
        "    \"\"\"\n",
        "    mean = mean[None, :, None, None]\n",
        "    std = std[None, :, None, None]\n",
        "    return tensor.sub(mean).div(std)\n",
        "\n",
        "\n",
        "'''\n",
        "Load pre-trained ImageNet models\n",
        "\n",
        "For models pre-trained on Stylized-ImageNet:\n",
        "[ICLR 2019] ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness\n",
        "Paper: https://openreview.net/forum?id=Bygh9j09KX\n",
        "Code: https://github.com/rgeirhos/texture-vs-shape\n",
        "'''    \n",
        "def model_imgnet(model_name):\n",
        "    '''\n",
        "    model_name options:\n",
        "    resnet50_SIN       trained on Stylized only\n",
        "    resnet50_SIN-IN    trained on ImageNet + Stylized\n",
        "    resnet50_SIN-2IN   trained on ImageNet + Stylized, then fine-tuned on ImageNet\n",
        "    \n",
        "    or load torchvision.models pre-trained on ImageNet: https://pytorch.org/docs/stable/torchvision/models.html\n",
        "    '''\n",
        "    \n",
        "    if model_name[:12] == 'resnet50_SIN':\n",
        "        model_urls = {\n",
        "            'resnet50_SIN': 'https://bitbucket.org/robert_geirhos/texture-vs-shape-pretrained-models/raw/6f41d2e86fc60566f78de64ecff35cc61eb6436f/resnet50_train_60_epochs-c8e5653e.pth.tar',\n",
        "            'resnet50_SIN-IN': 'https://bitbucket.org/robert_geirhos/texture-vs-shape-pretrained-models/raw/60b770e128fffcbd8562a3ab3546c1a735432d03/resnet50_train_45_epochs_combined_IN_SF-2a0d100e.pth.tar',\n",
        "            'resnet50_SIN-2IN': 'https://bitbucket.org/robert_geirhos/texture-vs-shape-pretrained-models/raw/60b770e128fffcbd8562a3ab3546c1a735432d03/resnet50_finetune_60_epochs_lr_decay_after_30_start_resnet50_train_45_epochs_combined_IN_SF-ca06340c.pth.tar',\n",
        "        }\n",
        "        model = torchvision.models.resnet50(pretrained=False)\n",
        "        model = nn.DataParallel(model).cuda()\n",
        "        checkpoint = model_zoo.load_url(model_urls[model_name])\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        \n",
        "    # Load pre-trained ImageNet models from torchvision\n",
        "    else:\n",
        "        model = eval(\"torchvision.models.{}(pretrained=True)\".format(model_name))\n",
        "        model = nn.DataParallel(model).cuda()\n",
        "    \n",
        "    # Normalization wrapper, so that we don't have to normalize adversarial perturbations\n",
        "    normalize = Normalizer(mean = IMGNET_MEAN, std = IMGNET_STD)\n",
        "    model = nn.Sequential(normalize, model)\n",
        "    model = model.cuda()\n",
        "    print(\"Model loading complete.\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "# Load pre-trained CIFAR-10 models\n",
        "def model_cifar(model_name, ckpt_path):\n",
        "    '''\n",
        "    CIFAR-10 model implementations from:\n",
        "    https://github.com/kuangliu/pytorch-cifar\n",
        "    '''\n",
        "    if model_name == 'resnet18':\n",
        "        model = ResNet18()\n",
        "    elif model_name == 'vgg16':\n",
        "        model = VGG('VGG16')\n",
        "        \n",
        "    model = model.cuda()\n",
        "    model = torch.nn.DataParallel(model)\n",
        "    \n",
        "    # Load saved weights and stats\n",
        "    checkpoint = torch.load(ckpt_path)\n",
        "    model.load_state_dict(checkpoint['net'])\n",
        "    best_acc = checkpoint['acc']\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    \n",
        "    # Normalization wrapper, so that we don't have to normalize adversarial perturbations\n",
        "    normalize = Normalizer(mean = CIFAR_MEAN, std = CIFAR_STD)\n",
        "    model = nn.Sequential(normalize, model)\n",
        "    model = model.cuda()\n",
        "\n",
        "    return model, best_acc\n",
        "\n",
        "\n",
        "# dataloader for ImageNet\n",
        "def loader_imgnet(dir_data, nb_images = 50000, batch_size = 100, img_size = 224):\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(img_size),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    \n",
        "    val_dataset = ImageFolder(dir_data, val_transform)\n",
        "    \n",
        "    # Random subset if not using the full 50,000 validation set\n",
        "    if nb_images < 50000:\n",
        "        np.random.seed(0)\n",
        "        sample_indices = np.random.permutation(range(50000))[:nb_images]\n",
        "        val_dataset = Subset(val_dataset, sample_indices)\n",
        "    \n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size = batch_size,                              \n",
        "        shuffle = True, \n",
        "        num_workers = max(1, multiprocessing.cpu_count() - 1)\n",
        "    )\n",
        "    \n",
        "    return dataloader\n",
        "\n",
        "\n",
        "# dataloader for CIFAR-10\n",
        "def loader_cifar(dir_data, train = False, batch_size = 250):\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "    if train:\n",
        "        trainset = torchvision.datasets.CIFAR10(root = dir_data, train = True, download = True, transform = transform_test)\n",
        "        dataloader = torch.utils.data.DataLoader(trainset, batch_size = batch_size, shuffle = True, num_workers = max(1, multiprocessing.cpu_count() - 1))\n",
        "    else:\n",
        "        testset = torchvision.datasets.CIFAR10(root = dir_data, train = False, download = True, transform = transform_test)\n",
        "        dataloader = torch.utils.data.DataLoader(testset, batch_size = batch_size, shuffle = True, num_workers = max(1, multiprocessing.cpu_count() - 1))\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "# Evaluate model on data with or without UAP\n",
        "# Assumes data range is bounded by [0, 1]\n",
        "def evaluate(model, loader, uap = None, n = 5):\n",
        "    '''\n",
        "    OUTPUT\n",
        "    top         top n predicted labels (default n = 5)\n",
        "    top_probs   top n probabilities (default n = 5)\n",
        "    top1acc     array of true/false if true label is in top 1 prediction\n",
        "    top5acc     array of true/false if true label is in top 5 prediction\n",
        "    outputs     output labels\n",
        "    labels      true labels\n",
        "    '''\n",
        "    probs, labels = [], []\n",
        "    model.eval()\n",
        "    \n",
        "    if uap is not None:\n",
        "        _, (x_val, y_val) = next(enumerate(loader))\n",
        "        batch_size = len(x_val)\n",
        "        uap = uap.unsqueeze(0).repeat([batch_size, 1, 1, 1])\n",
        "    \n",
        "    with torch.set_grad_enabled(False):\n",
        "        for i, (x_val, y_val) in enumerate(loader):\n",
        "            \n",
        "            if uap is None:\n",
        "                out = torch.nn.functional.softmax(model(x_val.cuda()), dim = 1)\n",
        "            else:\n",
        "                perturbed = torch.clamp((x_val + uap).cuda(), 0, 1) # clamp to [0, 1]\n",
        "                out = torch.nn.functional.softmax(model(perturbed), dim = 1)\n",
        "                \n",
        "            probs.append(out.cpu().numpy())\n",
        "            labels.append(y_val)\n",
        "            \n",
        "    # Convert batches to single numpy arrays    \n",
        "    probs = np.stack([p for l in probs for p in l])\n",
        "    labels = np.array([t for l in labels for t in l])\n",
        "    \n",
        "    # Extract top 5 predictions for each example\n",
        "    top = np.argpartition(-probs, n, axis = 1)[:,:n]\n",
        "    top_probs = probs[np.arange(probs.shape[0])[:, None], top].astype(np.float16)\n",
        "    top1acc = top[range(len(top)), np.argmax(top_probs, axis = 1)] == labels\n",
        "    top5acc = [labels[i] in row for i, row in enumerate(top)]\n",
        "    outputs = top[range(len(top)), np.argmax(top_probs, axis = 1)]\n",
        "        \n",
        "    return top, top_probs, top1acc, top5acc, outputs, labels\n"
      ],
      "metadata": {
        "id": "rsAU_XFZlC97"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def uap_sgd(model, loader, nb_epoch, eps, beta = 12, step_decay = 0.8, y_target = None, loss_fn = None, layer_name = None, uap_init = None):\n",
        "    '''\n",
        "    INPUT\n",
        "    model       model\n",
        "    loader      dataloader\n",
        "    nb_epoch    number of optimization epochs\n",
        "    eps         maximum perturbation value (L-infinity) norm\n",
        "    beta        clamping value\n",
        "    y_target    target class label for Targeted UAP variation\n",
        "    loss_fn     custom loss function (default is CrossEntropyLoss)\n",
        "    layer_name  target layer name for layer maximization attack\n",
        "    uap_init    custom perturbation to start from (default is random vector with pixel values {-eps, eps})\n",
        "    \n",
        "    OUTPUT\n",
        "    delta.data  adversarial perturbation\n",
        "    losses      losses per iteration\n",
        "    '''\n",
        "    _, (x_val, y_val) = next(enumerate(loader))\n",
        "    batch_size = len(x_val)\n",
        "    if uap_init is None:\n",
        "        batch_delta = torch.zeros_like(x_val, device=\"cuda\", requires_grad=True) # initialize as zero vector\n",
        "        batch_delta.retain_grad()\n",
        "    else:\n",
        "        batch_delta = uap_init.unsqueeze(0).repeat([batch_size, 1, 1, 1])\n",
        "    delta = batch_delta[0]\n",
        "    losses = []\n",
        "    \n",
        "    # loss function\n",
        "    if layer_name is None:\n",
        "        if loss_fn is None: loss_fn = nn.CrossEntropyLoss(reduction = 'none')\n",
        "        beta = torch.cuda.FloatTensor([beta])\n",
        "        def clamped_loss(output, target):\n",
        "            loss = torch.mean(torch.min(loss_fn(output, target), beta))\n",
        "            return loss\n",
        "       \n",
        "    # layer maximization attack\n",
        "    else:\n",
        "        def get_norm(self, forward_input, forward_output):\n",
        "            global main_value\n",
        "            main_value = torch.norm(forward_output, p = 'fro')\n",
        "        for name, layer in model.named_modules():\n",
        "            if name == layer_name:\n",
        "                handle = layer.register_forward_hook(get_norm)\n",
        "                \n",
        "    batch_delta.requires_grad_()\n",
        "\n",
        "    print(batch_delta.grad, type(batch_delta))\n",
        "    for epoch in range(nb_epoch):\n",
        "        print('epoch %i/%i' % (epoch + 1, nb_epoch))\n",
        "        \n",
        "        # perturbation step size with decay\n",
        "        eps_step = eps * step_decay\n",
        "        \n",
        "        for i, (x_val, y_val) in enumerate(loader):\n",
        "            batch_delta.grad.data.zero_()\n",
        "            batch_delta.data = delta.unsqueeze(0).repeat([x_val.shape[0], 1, 1, 1])\n",
        "\n",
        "            # for targeted UAP, switch output labels to y_target\n",
        "            if y_target is not None: y_val = torch.ones(size = y_val.shape, dtype = y_val.dtype) * y_target\n",
        "            \n",
        "            perturbed = torch.clamp((x_val + batch_delta).cuda(), 0, 1)\n",
        "            outputs = model(perturbed)\n",
        "            \n",
        "            # loss function value\n",
        "            if layer_name is None: loss = clamped_loss(outputs, y_val.cuda())\n",
        "            else: loss = main_value\n",
        "            \n",
        "            if y_target is not None: loss = -loss # minimize loss for targeted UAP\n",
        "            losses.append(torch.mean(loss))\n",
        "            loss.backward()\n",
        "            \n",
        "            # batch update\n",
        "            grad_sign = batch_delta.grad.data.mean(dim = 0).sign()\n",
        "            delta = delta + grad_sign * eps_step\n",
        "            delta = torch.clamp(delta, -eps, eps)\n",
        "            batch_delta.grad.data.zero_()\n",
        "    \n",
        "    if layer_name is not None: handle.remove() # release hook\n",
        "    \n",
        "    return delta.data, losses"
      ],
      "metadata": {
        "id": "BHFZbK0aK_BH"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CIFAR"
      ],
      "metadata": {
        "id": "S_j7YvYYxYQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown --id 1HimV7njf1XYwdiFi4okT-IGYL5sqidWy\n",
        "# gdown.download_folder('https://drive.google.com/drive/folders/1HimV7njf1XYwdiFi4okT-IGYL5sqidWy?usp=share_link', quiet=True)"
      ],
      "metadata": {
        "id": "HZhe_rM5gV7H"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testloader = loader_cifar(dir_data = 'cifar10', train = False)"
      ],
      "metadata": {
        "id": "T05MUfjIhp9O"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainloader = loader_cifar(dir_data = 'cifar10', train = True)"
      ],
      "metadata": {
        "id": "OjlCuB_4iyeR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown --id 1Aj0Q3loc2adoJDkB_2jQqMArMBu-gurS\n",
        "# gdown.download('https://drive.google.com/file/d/1Aj0Q3loc2adoJDkB_2jQqMArMBu-gurS', 'resnet18.pth', quiet=True)"
      ],
      "metadata": {
        "id": "QIWR_FiPoCVH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model, best_acc = model_cifar('resnet18', ckpt_path = 'resnet18.pth')"
      ],
      "metadata": {
        "id": "UHynC6_lkhla"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ImageNet"
      ],
      "metadata": {
        "id": "OcfggOWoxfHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir_data = 'data/'\n",
        "dir_uap = 'uaps/imagenet/'"
      ],
      "metadata": {
        "id": "_AcSGSEClQcq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mH7n3kgPxqy-",
        "outputId": "dfcca861-6e64-4647-c9f5-c272d6eefbb7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-26 05:07:12--  https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar\n",
            "Resolving image-net.org (image-net.org)... 171.64.68.16\n",
            "Connecting to image-net.org (image-net.org)|171.64.68.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6744924160 (6.3G) [application/x-tar]\n",
            "Saving to: ‘ILSVRC2012_img_val.tar’\n",
            "\n",
            "ILSVRC2012_img_val. 100%[===================>]   6.28G  59.6MB/s    in 2m 54s  \n",
            "\n",
            "2023-02-26 05:10:06 (36.9 MB/s) - ‘ILSVRC2012_img_val.tar’ saved [6744924160/6744924160]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data"
      ],
      "metadata": {
        "id": "4iwQrVYd9hvA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xf  'ILSVRC2012_img_val.tar' -C 'data/'"
      ],
      "metadata": {
        "id": "ZjP-P1rZxlzn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"data/\"\n",
        "# !ls"
      ],
      "metadata": {
        "id": "7CzZxCRh_cBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -qO- https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh | bash"
      ],
      "metadata": {
        "id": "RI5haKHwJcsx"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !ls"
      ],
      "metadata": {
        "id": "6a2h688oJ0wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKgGjFHLJsZE",
        "outputId": "f80bb53d-3493-46d2-8ab9-c6debc678013"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = loader_imgnet(dir_data, 2000, 100)"
      ],
      "metadata": {
        "id": "ydlbHqcYyPjI"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_imgnet('resnet50')"
      ],
      "metadata": {
        "id": "HYfS8WSp932P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180,
          "referenced_widgets": [
            "ad59464a7a794481ad79d847454eaeb9",
            "51c90702ca8247b3b5185fbcc7237d53",
            "649700260eb941b69153b610c0987268",
            "72417829d97f4c19a311c5b5347e53aa",
            "0e9d618aa77d4b04922e59017caa0e6d",
            "66ce74ff5fbb4e0bbb2bfec02015cebd",
            "95a1cc895dcf4f4786d25696ee4dc68b",
            "ba491fc69141487b8484ccb2bf4fda17",
            "660e7914949549d4a5bfeb96bc2c22f4",
            "f145e7ce1aed411bb349b914b59a1ed2",
            "0b19fd438b424a0db1ccc25b5ded3422"
          ]
        },
        "outputId": "36ce2287-a23c-4ee6-a8f8-9f253a8e841c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad59464a7a794481ad79d847454eaeb9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loading complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean accuracy"
      ],
      "metadata": {
        "id": "Tq2XNTNiKkRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clean accuracy\n",
        "_, _, _, _, outputs, labels = evaluate(model, loader)\n",
        "print('Accuracy:', sum(outputs == labels) / len(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfk3hdF7Kghm",
        "outputId": "ed4578c4-098b-44ef-f945-c5ea38f06fae"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Untargeted UAP"
      ],
      "metadata": {
        "id": "a11pfcfWK0oV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_epoch = 10\n",
        "eps = 10 / 255\n",
        "beta = 12\n",
        "step_decay = 0.7\n",
        "uap, losses = uap_sgd(model, loader, nb_epoch, eps, beta, step_decay)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "KUqmFzuVKpyt",
        "outputId": "948d68e2-c109-4308-8a6a-b65614b813b0"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None <class 'torch.Tensor'>\n",
            "epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-500942948159>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstep_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0muap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muap_sgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-79-e2a32c7c8339>\u001b[0m in \u001b[0;36muap_sgd\u001b[0;34m(model, loader, nb_epoch, eps, beta, step_decay, y_target, loss_fn, layer_name, uap_init)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mbatch_delta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mbatch_delta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6srtXfkWK2nr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}